# Multi-LLM Development Framework - Environment Configuration
# Copy to .env and fill in your values

# === API Keys ===
# Claude API (required for cloud routing)
ANTHROPIC_API_KEY=sk-ant-xxxxx

# OpenAI (optional, for GPT models)
OPENAI_API_KEY=sk-xxxxx

# LiteLLM master key (for proxy authentication)
LITELLM_MASTER_KEY=sk-litellm-xxxxx

# === Local Model Paths ===
# llama.cpp executable path
LLAMA_CPP_PATH=D:/path/to/llama-cpp/llama-server.exe

# Model files directory
MODELS_DIR=D:/path/to/models

# === Service Configuration ===
# LiteLLM proxy
LITELLM_HOST=127.0.0.1
LITELLM_PORT=4000

# Policy Router
ROUTER_HOST=0.0.0.0
ROUTER_PORT=5000

# vLLM server
VLLM_HOST=127.0.0.1
VLLM_PORT=8000

# llama.cpp server
LLAMA_CPP_HOST=127.0.0.1
LLAMA_CPP_PORT=8080

# === Multi-LLM Directory ===
MULTI_LLM_DIR=.multi-llm

# === Logging ===
LOG_LEVEL=INFO
LOG_FILE=.multi-llm/logs/app.log

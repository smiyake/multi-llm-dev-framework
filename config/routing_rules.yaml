# Policy Router Rules - GPT-OSS-20B Unified (16GB VRAM Environment)
# Define task-to-model routing logic

# 2-Stage Strategy:
# Stage 1 (Now - 16GB): GPT-OSS-20B (Q4) unified for all local tasks
# Stage 2 (Future - 96GB): Add Qwen3-32B for Japanese sentiment analysis

# Default fallback chain
fallback_chain:
  - gpt-oss-20b-q4     # Primary: 16GB optimized
  - claude-sonnet      # Fallback: Cloud API

# Task type routing rules
rules:
  # Sentiment analysis -> GPT-OSS-20B (Q4)
  - task_type: sentiment
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Unified local model for 16GB VRAM"
    output_schema:
      - sentiment      # positive/negative/neutral
      - confidence     # 0.0-1.0
      - rationale      # brief explanation
      - model_id       # for A/B testing

  # Log summary -> GPT-OSS-20B (Q4)
  - task_type: log_summary
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Bulk processing, consistent quality"

  # Code generation -> GPT-OSS-20B (Q4)
  - task_type: code_gen
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Local code generation, no API cost"

  # Document generation -> GPT-OSS-20B (Q4)
  - task_type: document
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Pattern-based generation, local processing"

  # Analysis tasks -> GPT-OSS-20B (Q4)
  - task_type: analysis
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Data analysis, local inference"

  # Agent support (Cline, Aider, OpenHands) -> GPT-OSS-20B (Q4)
  - task_type: agent_support
    model: gpt-oss-20b-q4
    endpoint: direct
    reason: "Coding agent backend, high throughput"

  # High-quality review -> Claude (Cloud)
  - task_type: code_review
    model: claude-sonnet
    endpoint: litellm
    reason: "High quality review requires advanced reasoning"

  # Architecture decisions -> Claude (Cloud)
  - task_type: architecture
    model: claude-sonnet
    endpoint: litellm
    reason: "Complex reasoning required"

  # Security audit -> Claude (Cloud)
  - task_type: security_audit
    model: claude-sonnet
    endpoint: litellm
    reason: "Critical decision making, high stakes"

# Resource-based rules
resource_rules:
  # When GPU VRAM < 4GB, use quantized model
  - condition: "gpu_free_mb < 4000"
    action: "use_q4_quantized"
    reason: "GPU memory limited, use Q4 quantization"

  # When GPU unavailable, fallback to cloud
  - condition: "gpu_available == false"
    action: "use_cloud"
    reason: "No GPU, fallback to Claude API"

# Cost optimization
cost_rules:
  # Batch tasks -> always local
  - condition: "batch_size > 5"
    action: "use_local"
    reason: "Cost optimization for batch processing"

  # Night time -> aggressive local usage
  - condition: "hour >= 22 or hour <= 6"
    action: "prefer_local"
    reason: "Night batch processing, no latency requirements"

# Future expansion (96GB VRAM)
# future_rules:
#   - task_type: sentiment_japanese
#     model: qwen3-32b
#     endpoint: direct
#     reason: "Japanese-optimized sentiment analysis"
